{"units": 15, "layers": 6, "activation": "tanh", "optimizer": "adam", "epochs": 5, "batch_size": 64, "loss": "mse"}