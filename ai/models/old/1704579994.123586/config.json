{"units": 8, "layers": 3, "activation": "tanh", "optimizer": "adam", "epochs": 5, "batch_size": 64, "loss": "mse"}